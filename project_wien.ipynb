{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "unsigned-invalid",
   "metadata": {},
   "source": [
    "# Schmerzgrenze der Wiener üöâ\n",
    "\n",
    "<img src=\"https://www.biorama.eu/wp-content/uploads/2016/02/Bildschirmfoto-2016-02-26-um-17.14.57.png\"></img>\n",
    "\n",
    "### Project Aim\n",
    "This Project aims to correlate, visualize and find patterns regarding the developments in the Viennese public transport grid and gauging public sentiment in correlation to such incidents. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adverse-michigan",
   "metadata": {},
   "source": [
    "\n",
    "### Team Members\n",
    "Julian Deleja-Hotko\\\n",
    "Nicolas Markl\\\n",
    "Dionis Ramadani"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "naval-stream",
   "metadata": {},
   "source": [
    "### Data Sources\n",
    "**// TODO: DIE VERSCHIEDENEN DATEN GENAUER BESCHREIBEN**\n",
    "\n",
    "##### [Digitales Wien / Open Government Data Portal Wien](https://digitales.wien.gv.at/open-data/) - REST Endpoint\n",
    "Several hundred data sets provide detailed information about one-way streets, real-time information from Wiener Linien, historical aerial photographs, measurement data of air pollutants or WLAN locations, to name just a few areas.\n",
    "\n",
    "##### [√ñffi.at](√∂ffi.at)  - XML / Web Scraping\n",
    "A website gathering and organizing historical data about Wiener Linien outages, courtesy of Klaus Kirnbauer.\\\n",
    "Data available starting from July 2020.\n",
    "\n",
    "##### [Twitter@WienerLinien](https://twitter.com/wienerlinien) - Web Scraping\n",
    "A social media platform popular in Vienna with dedicated accounts from public service providers, useful for gauging sentiment about specific routes in Vienna\n",
    "\n",
    "##### [data.gv.at](data.gv.at) - Flat Files\n",
    "Gathering general info about the public transport network, usage, etc.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "greater-pension",
   "metadata": {},
   "source": [
    "### Architecture Diagram\n",
    "**//TODO: Hier Architekturdiagramm (verpflichtend)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "silver-cooking",
   "metadata": {},
   "source": [
    "### Packages\n",
    "Here we'll install and import all relevant Python packages for this project"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 3,
=======
   "execution_count": 1,
>>>>>>> 890e97e576f4939257898a3450013e8987992235
   "id": "dietary-coalition",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "Collecting geopandas\n",
      "  Using cached geopandas-0.11.0-py3-none-any.whl (1.0 MB)\n",
      "Requirement already satisfied: shapely<2,>=1.7 in c:\\users\\notyo\\anaconda3\\lib\\site-packages (from geopandas) (1.8.2)\n",
      "Requirement already satisfied: pyproj>=2.6.1.post1 in c:\\users\\notyo\\anaconda3\\lib\\site-packages (from geopandas) (3.3.1)\n",
      "Requirement already satisfied: packaging in c:\\users\\notyo\\anaconda3\\lib\\site-packages (from geopandas) (21.0)\n",
      "Requirement already satisfied: pandas>=1.0.0 in c:\\users\\notyo\\anaconda3\\lib\\site-packages (from geopandas) (1.4.3)\n",
      "Requirement already satisfied: fiona>=1.8 in c:\\users\\notyo\\anaconda3\\lib\\site-packages (from geopandas) (1.8.21)\n",
      "Requirement already satisfied: attrs>=17 in c:\\users\\notyo\\anaconda3\\lib\\site-packages (from fiona>=1.8->geopandas) (21.2.0)\n",
      "Requirement already satisfied: click>=4.0 in c:\\users\\notyo\\anaconda3\\lib\\site-packages (from fiona>=1.8->geopandas) (8.0.3)\n",
      "Requirement already satisfied: munch in c:\\users\\notyo\\anaconda3\\lib\\site-packages (from fiona>=1.8->geopandas) (2.5.0)\n",
      "Requirement already satisfied: gdal~=3.4.1 in c:\\users\\notyo\\anaconda3\\lib\\site-packages (from fiona>=1.8->geopandas) (3.4.3)\n",
      "Requirement already satisfied: six>=1.7 in c:\\users\\notyo\\anaconda3\\lib\\site-packages (from fiona>=1.8->geopandas) (1.16.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\notyo\\anaconda3\\lib\\site-packages (from fiona>=1.8->geopandas) (2021.10.8)\n",
      "Requirement already satisfied: click-plugins>=1.0 in c:\\users\\notyo\\anaconda3\\lib\\site-packages (from fiona>=1.8->geopandas) (1.1.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\notyo\\anaconda3\\lib\\site-packages (from fiona>=1.8->geopandas) (58.0.4)\n",
      "Requirement already satisfied: cligj>=0.5 in c:\\users\\notyo\\anaconda3\\lib\\site-packages (from fiona>=1.8->geopandas) (0.7.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\notyo\\anaconda3\\lib\\site-packages (from click>=4.0->fiona>=1.8->geopandas) (0.4.4)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\notyo\\anaconda3\\lib\\site-packages (from pandas>=1.0.0->geopandas) (2021.3)\n",
      "Requirement already satisfied: numpy>=1.18.5 in c:\\users\\notyo\\anaconda3\\lib\\site-packages (from pandas>=1.0.0->geopandas) (1.22.4+vanilla)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\notyo\\anaconda3\\lib\\site-packages (from pandas>=1.0.0->geopandas) (2.8.2)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\notyo\\anaconda3\\lib\\site-packages (from packaging->geopandas) (3.0.4)\n",
      "Installing collected packages: geopandas\n",
      "Successfully installed geopandas-0.11.0\n"
=======
      "Collecting pymongo\n",
      "  Downloading pymongo-4.1.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (471 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m471.3/471.3 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m0:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pymongo\n",
      "Successfully installed pymongo-4.1.1\n",
      "Requirement already satisfied: pyspark in /home/julian/anaconda3/lib/python3.9/site-packages (3.2.1)\n",
      "Requirement already satisfied: py4j==0.10.9.3 in /home/julian/anaconda3/lib/python3.9/site-packages (from pyspark) (0.10.9.3)\n",
      "Requirement already satisfied: requests in /home/julian/anaconda3/lib/python3.9/site-packages (2.27.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/julian/anaconda3/lib/python3.9/site-packages (from requests) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/julian/anaconda3/lib/python3.9/site-packages (from requests) (1.26.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/julian/anaconda3/lib/python3.9/site-packages (from requests) (3.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/julian/anaconda3/lib/python3.9/site-packages (from requests) (2.0.4)\n",
      "Requirement already satisfied: beautifulsoup4 in /home/julian/anaconda3/lib/python3.9/site-packages (4.11.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in /home/julian/anaconda3/lib/python3.9/site-packages (from beautifulsoup4) (2.3.1)\n",
      "Requirement already satisfied: pyspark in /home/julian/anaconda3/lib/python3.9/site-packages (3.2.1)\n",
      "Requirement already satisfied: py4j==0.10.9.3 in /home/julian/anaconda3/lib/python3.9/site-packages (from pyspark) (0.10.9.3)\n",
      "Requirement already satisfied: pandas in /home/julian/anaconda3/lib/python3.9/site-packages (1.4.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /home/julian/anaconda3/lib/python3.9/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/julian/anaconda3/lib/python3.9/site-packages (from pandas) (2021.3)\n",
      "Requirement already satisfied: numpy>=1.18.5 in /home/julian/anaconda3/lib/python3.9/site-packages (from pandas) (1.21.5)\n",
      "Requirement already satisfied: six>=1.5 in /home/julian/anaconda3/lib/python3.9/site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n"
>>>>>>> 890e97e576f4939257898a3450013e8987992235
     ]
    }
   ],
   "source": [
    "!pip install pymongo\n",
    "!pip install pyspark\n",
    "!pip install requests\n",
    "!pip install beautifulsoup4\n",
    "!pip install pyspark \n",
    "!pip install pandas\n",
    "!pip install geopandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "weekly-conditions",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "from pymongo import MongoClient\n",
    "from pyspark import SparkContext\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brown-demographic",
   "metadata": {},
   "source": [
    "### Gathering, Storing and Cleaning our Data\n",
    "The data will be collected and processed via Kafka, analyzed with Spark and all the relevant data will be stored on our MongoDB instance after an ETL-style pipeline.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "greek-bradley",
   "metadata": {},
   "source": [
    "##### Setting up DB connection\n",
    "We're connecting to our local MongoDB instance, this is to pipe our extracted and transformed data into the DB later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "possible-lobby",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'admin', 'sizeOnDisk': 40960, 'empty': False}\n",
      "{'name': 'config', 'sizeOnDisk': 36864, 'empty': False}\n",
      "{'name': 'hackernews', 'sizeOnDisk': 81920, 'empty': False}\n",
      "{'name': 'immodb', 'sizeOnDisk': 10588160, 'empty': False}\n",
      "{'name': 'local', 'sizeOnDisk': 409600, 'empty': False}\n",
      "{'name': 'wienerLinien', 'sizeOnDisk': 5537792, 'empty': False}\n"
     ]
    }
   ],
   "source": [
    "# Provide the mongodb connection string\n",
    "CONNECTION_STRING = 'mongodb://localhost:27017'\n",
    "\n",
    "# Create a connection using MongoClient\n",
    "myclient = MongoClient(CONNECTION_STRING)\n",
    "\n",
    "# Check DB collections\n",
    "for db in myclient.list_databases():\n",
    "    print(db)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d2d715",
   "metadata": {},
   "source": [
    "##### Scraping together historical Wiener Linien Data\n",
    "For this step, we're using the √ñffi.at website by Klaus Kirnbauer who has aggregated all historical Wiener Linien public transport incidents in an easily queriable fashion.\n",
    "\n",
    "Since √ñffi.at uses conveniently utilizes server-side rendering, we can use BeautifulSoup for our data transformation. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20dc7515",
   "metadata": {},
   "source": [
    "Now first, we need to model a framework of parsing the relevant data from the 1520 available historical sites;\\\n",
    "Since the data is variable in some cases we have decided on the following format:\n",
    "\n",
    "| [Affected Lines] | [Affected Stations] | Start Time | End Time | Time Problem Fixed | Title |\n",
    "|------------------|---------------------|------------|----------|--------------------|-------|\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "9da174ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format\n",
    "# ([Affected Lines], [Affected Stations], Start Time, End Time, Time Problem Fixed, Title)\n",
    "def parse_oeffi_soup(soup):\n",
    "    data = list(map(lambda li: (list(map(lambda trafficline: trafficline.getText(), li.select('.trafficline'))), \n",
    "                                 list(map(lambda liSub: liSub.split('<li>')[1],\n",
    "                                     list(filter(re.compile('[^+]*\\n<b>Von</b>:.').match, str(li).split('<br/>')))[0].split('</li>')[0:-1])),\n",
    "                                 list(filter(re.compile('[^+]*\\n<b>Von</b>:.').match, str(li).split('<br/>')))[0].split('<b>Von</b>: ')[1],\n",
    "                                 list(filter(re.compile('\\n<b>Bis</b>:').match, str(li).split('<br/>'))),\n",
    "                                 list(filter(re.compile('\\n<b>Verkehrsaufnahme</b>:').match, str(li).split('<br/>'))),\n",
    "                                 li.select('.disruption-title')[0].getText()),\n",
    "                     soup.select('li.disruption')))\n",
    "\n",
    "    return list(map(lambda x: (x[0], x[1], x[2], x[3][0].split('</b>: ')[1] if len(x[3]) > 0 else None, x[4][0].split('</b>: ')[1] if len(x[4]) > 0 else None, x[5]), data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d78a8e1f",
   "metadata": {},
   "source": [
    "And now we can run this model on all the available sites and aggregate this data! \\\n",
    "For estimation, this takes around 15-20 minutes to run to completion with all 1520 requests."
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 15,
=======
   "execution_count": null,
>>>>>>> 890e97e576f4939257898a3450013e8987992235
   "id": "25ffa2c5",
   "metadata": {},
   "outputs": [
    {
<<<<<<< HEAD
     "ename": "TypeError",
     "evalue": "'Collection' object is not callable. If you meant to call the 'head' method on a 'Collection' object it is failing because no such method exists.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_17952/2290814794.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mstoerungen_col\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdb\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'stoerungen'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstoerungen_col\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pymongo\\collection.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3164\u001b[0m                 \u001b[1;34m\"exists.\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3165\u001b[0m             )\n\u001b[1;32m-> 3166\u001b[1;33m         raise TypeError(\n\u001b[0m\u001b[0;32m   3167\u001b[0m             \u001b[1;34m\"'Collection' object is not callable. If you meant to \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3168\u001b[0m             \u001b[1;34m\"call the '%s' method on a 'Collection' object it is \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'Collection' object is not callable. If you meant to call the 'head' method on a 'Collection' object it is failing because no such method exists."
=======
     "ename": "NameError",
     "evalue": "name 'requests' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1520\u001b[39m):\n\u001b[1;32m      4\u001b[0m     URL \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://xn--ffi-rna.at/?archive=1&page=\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(i)\n\u001b[0;32m----> 5\u001b[0m     page \u001b[38;5;241m=\u001b[39m \u001b[43mrequests\u001b[49m\u001b[38;5;241m.\u001b[39mget(URL)\n\u001b[1;32m      6\u001b[0m     soup \u001b[38;5;241m=\u001b[39m BeautifulSoup(page\u001b[38;5;241m.\u001b[39mcontent, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhtml.parser\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      7\u001b[0m     data\u001b[38;5;241m.\u001b[39mextend(parse_oeffi_soup(soup))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'requests' is not defined"
>>>>>>> 890e97e576f4939257898a3450013e8987992235
     ]
    }
   ],
   "source": [
    "data = []\n",
    "\n",
    "for i in range(1, 1520):\n",
    "    URL = 'https://xn--ffi-rna.at/?archive=1&page=' + str(i)\n",
    "    page = requests.get(URL)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    data.extend(parse_oeffi_soup(soup))\n",
    "    \n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de6c23d",
   "metadata": {},
   "source": [
    "After arduously gathering and cleaning our data, we'll now convert it to a Dataframe and insert it into our Mongo DB instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "3370ec52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pymongo.results.InsertManyResult at 0x15c7d3d1f70>"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Transform into Pandas DF\n",
    "df = pd.DataFrame(data, columns=['Affected Lines', 'Affected Stations', 'Start Time', 'End Time', 'Fixed Time', 'Title'])\n",
    "\n",
    "# Create Database\n",
    "db = myclient['wienerLinien']\n",
    "\n",
    "# Insert\n",
    "db.stoerungen.insert_many(df.to_dict('records'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "322f9342",
   "metadata": {},
   "source": [
    "Now we can also check if we have inserted our data correctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "63a75630",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stoerungen:  27088\n"
     ]
    }
   ],
   "source": [
    "stoerungen_col = db['stoerungen']\n",
    "print('Stoerungen: ', len(list(stoerungen_col.find())))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bbd48fa",
   "metadata": {},
   "source": [
    "##### Gathering Geodata about Stations\n",
    "For this step we read in a data.gv.at flat file for further usage in visualizing and interpreting the data:\n",
    "\n",
    "// TODO: Read in Shapefile, prepare for drawing of map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "212b8b09",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4f30fbf9",
   "metadata": {},
   "source": [
    "##### Gathering Twitter data "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d8f02a",
   "metadata": {},
   "source": [
    "For XYZ reasons we're using the Twitter API to gather XYZ as follows:\n",
    "\n",
    "// TODO: Write Twitter API code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "270f332f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "reported-victoria",
   "metadata": {},
   "source": [
    "##### Setting up Spark via SparkContext for MongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "extraordinary-school",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"MongoSparkConnector\") \\\n",
    "    .config(\"spark.mongodb.input.uri\", \"mongodb://localhost:27017/wienerLinien.stoerungen\") \\\n",
    "    .config(\"spark.mongodb.output.uri\", \"mongodb://localhost:27017/wienerLinien.stoerungen\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d502c5c-a838-4ca6-b427-d067d728cad6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b68ff89-be53-4566-b02e-f9d50f590a27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@LandauDaniel @wienerlinien @Tom_Harb Beim Hund im Auto kommt die @LPDWien und schl√§gt die Fenster ein. In Wien werben die @wienerlinien damit, dass in nur 20min eine klimatisierte Tram kommt. Vl, kommt auf die Linie drauf an und dann ist eine St√∂rung (wie immer)\n",
      "@oebb die frage geht auch an @wienerlinien weil wien mobil zeigt keine st√∂rung\n",
      "@wienerlinien Gibt es einen Grund daf√ºr, dass die 44 Richtung Schottentor in den letzten Wochen &amp; Monaten so unfassbar unzuverl√§ssig f√§hrt? Laut Wien Mobil App keine St√∂rung, sollte normal alle 7min fahren, an der Haltestelle steht kommt erst in 15min.. So macht das keinen Spa√ü‚òπÔ∏è https://t.co/FmW13vYhzv\n"
     ]
    }
   ],
   "source": [
    "import tweepy\n",
    "\n",
    "\n",
    "client=tweepy.Client(\"AAAAAAAAAAAAAAAAAAAAAHJMdgEAAAAAaazY9nw6SjvKprN4BDsBWVjfIZU%3DlzAqZ4fbC4UTH1caM2XLfZYlTjszcqASp43jAiDcBBO9Lsv08H\")\n",
    "public_tweets =client.search_recent_tweets('\"@WienerLinien\" (Ausfall OR St√∂rung)')\n",
    "\n",
    "#print(public_tweets[0])\n",
    "\n",
    "for tweet in public_tweets[0]:\n",
    "    print(tweet.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb0c5f7e-f55c-48ee-92a6-a1944897bcc3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "british-weather",
   "metadata": {},
   "source": [
    "\n",
    "### Analysis\n",
    "\n",
    "Graphs, maps and heatmaps showing / highlighting patterns and outages of the Viennese public transport system.\n",
    "\n",
    "##### Generating a Heat Map of most affected stations\n",
    "// TODO: Use the geolocation data and outage data in our DB to calculate the total amount of time a station was closed"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
